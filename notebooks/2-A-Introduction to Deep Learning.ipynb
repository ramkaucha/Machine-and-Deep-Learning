{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737a0eb4-a39c-4805-a264-78977dc39494",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "In machine learning a loss function is used to peanlise the model for learning parameters $\\theta$ that fail to approximate $F$ accurately. A loss function is similar to an evaluation metric, with the important difference: loss functions should always be differentiable, while evaluation metrics don't have to be differentiable.\n",
    "\n",
    "Gradient descent is how a machine learning model learns parameters $\\theta$ learns incrementally. Gradient descent only works for differentiable loss functions - find the local minimum of the loss function with respect to parameters. The optimisation method is called *gradient descent* because it involves differentiating the loss function with respect to each parameter to obtain the gradient, and then 'moving' these parameters in the direction of steepest descent on the loss function/surface.\n",
    "\n",
    "Recalling our formulation of single variable linear regress;\n",
    "$$\\hat{y} = f(X_i;theta) = \\theta_0 + \\theta_1x_1$$\n",
    "\n",
    "We can choose the following loss function since it differentiates nicely:\n",
    "$$L(x;\\theta) = \\frac{1}{2}(y-\\hat{y})^2$$\n",
    "\n",
    "The loss function measures the squared difference between predictions and actual values.\n",
    "\n",
    "Gradient of $L$ w.r.t $y$:\n",
    "$$\\frac{\\partial{L}}{\\partial{y}} = -(y - \\hat(y))$$\n",
    "\n",
    "Gradient of $L$ w.r.t $\\theta_1$:\n",
    "$$\\frac{\\partial{L}}{\\partial{\\theta_1}} = \\frac{\\partial{L}}{\\partial{\\hat{y}}} \\cdot \\frac{\\partial{\\hat{y}}}{\\partial{\\theta_1}} = -(y-\\hat{y})\\cdot x_1$$\n",
    "\n",
    "Gradient of $L$ w.r.t $\\theta_0$\n",
    "$$\\frac{\\partial{L}}{\\partial{\\theta_0}} = \\frac{\\partial{L}}{\\partial{\\theta_0}} \\cdot \\frac{\\partial{L}}{\\partial{\\theta_1}} = -(y - \\hat{y})$$\n",
    "\n",
    "We update the learnable parameters using their gradients and the learning data. The learning rate is tunable hyper-parameter $\\eta$.\n",
    "$$\\theta_1 = \\theta_1 - \\eta \\frac{\\partial{L}}{\\partial{\\theta_1}}$$\n",
    "$$\\theta_0 = \\theta_0 - \\eta \\frac{\\partial{L}}{\\partial{\\theta_0}}$$\n",
    "\n",
    "We often initialise the parameters $\\theta$ to a random value. In some cases we can use prior information to start somewhere better.\n",
    "\n",
    "## Simplified understanding\n",
    "Imagine being in a valley and blindfolded, goal is to reach the lowest point of the valley. How would you do it?\n",
    "1. Feel which way the ground slopes\n",
    "2. Take small steps downhill\n",
    "3. Repeat until you can't go any lower\n",
    "This is what *gradient descent* is, the 'valley' is your loss function (how wrong your model is), and you're trying to find the lowest point (where your model makes the fewest mistakes)\n",
    "\n",
    "### Key components\n",
    "Loss function: Measures hopw **wrong** models' prediction is\n",
    "Parameters ($\\theta$): Number oyur models adjusts to get better predictions\n",
    "Gradient; Slope that tells you which direction to move parameters\n",
    "Learning Rate ($\\eta$): How big of a step to take in that direction\n",
    "\n",
    "### Process\n",
    "1. Start with random parameters\n",
    "2. Make predictions with your model\n",
    "3. Calculate how wrong you are (loss)\n",
    "4. Find which direction to change parameters to reduce loss (gradient)\n",
    "5. Update parameters by taking a small step in that direction\n",
    "6. Repeat until loss stops decreasing\n",
    "\n",
    "![Title](../images/lr_gd.gif)\n",
    "\n",
    "#### Why gradient descent?\n",
    "Why not just solve for $\\frac{\\partial{L}}{\\partial{\\theta_1}} = \\frac{\\partial{L}}{\\partial{\\theta_0}} = 0$, to find the paraemters for the minimum of the loss function.\n",
    "\n",
    "In deep learning, it is common for there to be $1000$s of parameters in a model, which creates high-dimensional parameter spaces. It is usually computationally itractable or even impossible to calculate the global minimum of these loss functions. In very complex deep learning problems, we often settle for local minimum.\n",
    "\n",
    "<img src=\"../images/gd_multi_modal.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae5068-9464-49e0-9d08-abb8c3713019",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
