





# Importing the following

# provides support for large, multi-dimensional arrays and matrices
import numpy as np
# creates static, animated and interactive visualisations
import matplotlib.pyplot as plt
# provides high-level interface for stat graphics
import seaborn as sns
# datasets
from sklearn.datasets import load_diabetes, load_iris
# splits dataset into training and testing sets for model evaluation
from sklearn.model_selection import train_test_split
# Standardises features by removintg main and scaling to unit variance
from sklearn.preprocessing import StandardScaler
# multi-layer perceptron (neural network) for classification
from sklearn.neural_network import MLPClassifier
# tool for model evaluation
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
# modelling tools
from sklearn import linear_model


diabetes = load_diabetes()
print(f"Number of data sample: {diabetes.target.shape[0]}")
print(f"Number of potential feature variables: {diabetes.data.shape[1]}")
print(f"Feature variable names: {diabetes.feature_names}")


def single_variable_linear_regression(dataset, predictor_label):
    # index of the chosen feature (in this case, it was 'bp')
    i = dataset.feature_names.index(predictor_label)
    # Extract chosen feature column
    X = dataset.data[:,i]
    # Gets the target variable (disease progression)
    y = dataset.target

    # Splits data into tradining 0.6 and testing 0.4 (test_size) sets
    # random_state=5 to ensure reproducibiiy (wtf does this mean)
    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size = 0.4, random_state = 5)
    print(f"Number of training samples: {Xtr.shape[0]}")
    print(f"Number of testing samples: {Xte.shape[0]}")


    # Create linear regression object
    f = linear_model.LinearRegression()
    # train the model using the training sets
    # Reshapes X data to 2D array (required by sklearn) and fits the model
    f.fit(Xtr.reshape(-1, 1), ytr)
    
    # Makes predictions on test and training data
    yte_predicted = f.predict(Xte.reshape(-1, 1)) # test set predictions
    ytr_predicted = f.predict(Xtr.reshape(-1, 1)) # training set predictions

    # Learned parameters
    # 0 is interpect, 1 is slope
    print(f"Learned parameters ùúÉ_0: {f.intercept_}, ùúÉ_1: {f.coef_[0]}")

    # scatter plot of actual test data points
    plt.scatter(Xte, yte, color="black")
    # regression line
    plt.plot(Xte, yte_predicted, color="blue", linewidth=3)
    plt.xlabel(predictor_label)
    plt.ylabel("Disease Progression")
    plt.title("Linear Regression on Diabetes Dataset")
    # removes tick marks on axes
    plt.xticks(())
    plt.yticks(())
    plt.show()

    return ytr, yte, ytr_predicted, yte_predicted

# Feature variable options:
#age age in years
#bp average blood pressure
#s1 tc, total serum cholesterol
#s2 ldl, low-density lipoproteins
#s3 hdl, high-density lipoproteins
#s4 tch, total cholestrol / HDL
#s5 ltg, possibly log of serum triglycerides level
#s6 glu, blood sugar level

ytr, yte, ytr_pred, yte_pred = single_variable_linear_regression(diabetes, 'bp')





regression_metrics = [mean_squared_error, mean_absolute_error, mean_absolute_percentage_error]
for met in regression_metrics:
    print(f"Test {met.__name__}: %.2f" % met(yte, yte_pred))
    print(f"Train {met.__name__}: %.2f" % met(ytr, ytr_pred))

print(f"Test RMSE: %.2f" % mean_squared_error(yte, yte_pred, squared=False))
print(f"Train RMSE: %.2f" % mean_squared_error(ytr, ytr_pred, squared=False))





iris = load_iris()

print(f"Number of data samples: {iris.target.shape[0]}")
print(f"Number of potential feature variables: {iris.data.shape[1]}")
print(f"Feature variable names: {iris.feature_names}")
print(f"Target class labels: {iris.target_names}")

np.random.seed(1)

def add_noise(data, noise_level):
    # Peak-to-peak (maximum - minimum) along the given axis 
    feature_ranges = np.ptp(data, axis=0)
    # generate random hnoise with the same shape as the dataset
    noise = np.random.uniform(low=-noise_level * feature_ranges, high=noise_level * feature_ranges, size=data.shape)

    # Add noise to the dataset
    iris.data = iris.data + noise
    return iris.data

iris.data = add_noise(iris.data, 0.5)


X, y = iris.data, iris.target

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"Number of training samples: {Xtr.shape[0]}")
print(f"Number of testing sampels: {Xte.shape[0]}")

scaler = StandardScaler()
Xtr = scaler.fit_transform(Xtr)
Xte = scaler.transform(Xte)

# clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=42)

#overfitting the noise
# clf = MLPClassifier(hidden_layer_sizes=(400,), activation='relu', solver='adam', max_iter=1000, random_state=42)

# ignoring the noise
clf = MLPClassifier(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=1000, random_state=42)

clf.fit(Xtr, ytr)

# Predict the class labels for the testing set
yte_pred = clf.predict(Xte)
ytr_pred = clf.predict(Xtr)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(yte, yte_pred)
print("Accuracy:", accuracy)
feature_names = iris.feature_names
x_feature_index = 0 # index of the first feature
y_feature_index = 1 # index of the second feature

# Plot the data
cmap = ['blue', 'green', 'orange']
plt.figure(figsize=(10,6))
for class_label in np.unique(y):
    X_class = Xte[yte == class_label]
    plt.scatter(X_class[:, x_feature_index], X_class[:, y_feature_index], label=iris.target_names[class_label], color=cmap[class_label], s=180)


# Plot the predicted labels
for class_label in np.unique(y):
    X_class = Xte[yte_pred == class_label]
    plt.scatter(X_class[:, x_feature_index], X_class[:, y_feature_index], color=cmap[class_label], s=50)

plt.xlabel(feature_names[x_feature_index])
plt.ylabel(feature_names[y_feature_index])
plt.title('Actual vs Predicted Classification')
plt.legend()
plt.show()





print(f"Test accuracy: {accuracy_score(yte, yte_pred)}")
print(f"Train accuracy: {accuracy_score(ytr, ytr_pred)}\n")
print(f"Train precision: {precision_score(ytr, ytr_pred, average='macro')}")
print(f"Test precision: {precision_score(yte, yte_pred, average='macro')}")
print(f"Train recall: {recall_score(ytr, ytr_pred, average='macro')}")
print(f"Test recall: {recall_score(yte, yte_pred, average='macro')}")
print(f"Train f1 score: {f1_score(ytr, ytr_pred, average='macro')}")
print(f"Test f1 score: {f1_score(yte, yte_pred, average='macro')}")

conf_matrix = confusion_matrix(yte, yte_pred)

# Plot confusion matrix
plt.figure(figsize=(8,6))
sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=iris.target_names, yticklabels=iris.target_names)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()



